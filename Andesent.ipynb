{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Andesent.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "FFaUEOgllqMe",
        "colab_type": "code",
        "outputId": "4ac6f3f6-ac08-43d2-e195-610f37fa1803",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 131
        }
      },
      "source": [
        "from textblob import TextBlob\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "twXAiRVhov4Y",
        "colab_type": "code",
        "outputId": "8ed1e8cd-8738-4f48-bb25-d3e18bc961de",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "source": [
        "analysis = TextBlob(\"This table is black\")\n",
        "print(analysis.sentiment)\n",
        "\n",
        "print(analysis.tags)\n",
        "\n",
        "print(analysis.translate(to='es'))\n",
        "\n",
        "print(dir(analysis))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Sentiment(polarity=-0.16666666666666666, subjectivity=0.43333333333333335)\n",
            "[('This', 'DT'), ('table', 'NN'), ('is', 'VBZ'), ('black', 'JJ')]\n",
            "Esta mesa es negra\n",
            "['__add__', '__class__', '__contains__', '__delattr__', '__dict__', '__dir__', '__doc__', '__eq__', '__format__', '__ge__', '__getattribute__', '__getitem__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__iter__', '__le__', '__len__', '__lt__', '__module__', '__ne__', '__new__', '__reduce__', '__reduce_ex__', '__repr__', '__setattr__', '__sizeof__', '__str__', '__subclasshook__', '__weakref__', '_cmpkey', '_compare', '_create_sentence_objects', '_strkey', 'analyzer', 'classifier', 'classify', 'correct', 'detect_language', 'ends_with', 'endswith', 'find', 'format', 'index', 'join', 'json', 'lower', 'ngrams', 'noun_phrases', 'np_counts', 'np_extractor', 'parse', 'parser', 'polarity', 'pos_tagger', 'pos_tags', 'raw', 'raw_sentences', 'replace', 'rfind', 'rindex', 'sentences', 'sentiment', 'sentiment_assessments', 'serialized', 'split', 'starts_with', 'startswith', 'string', 'strip', 'stripped', 'subjectivity', 'tags', 'title', 'to_json', 'tokenize', 'tokenizer', 'tokens', 'translate', 'translator', 'upper', 'word_counts', 'words']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-OBwdSV3x-ZO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import files\n",
        "import io\n",
        "\n",
        "positivas = files.upload()\n",
        "positivas\n",
        "pos_count = 0\n",
        "pos_correct = 0\n",
        "with open(\"positivas.txt\",\"r\") as f:\n",
        "  for line in f.read().split('\\n'):\n",
        "    analysis = TextBlob(line)\n",
        "    #print(line)\n",
        "    try:\n",
        "      eng=analysis.translate(to='en')\n",
        "      if eng.sentiment.polarity > 0:\n",
        "        pos_correct += 1\n",
        "      pos_count +=1\n",
        "    except:\n",
        "      print (\"El elemento no está presente\")\n",
        "\n",
        "\n",
        "\n",
        "print(\"Precisión positiva = {}% via {} ejemplos\".format(pos_correct/pos_count*100.0, pos_count))\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K0ahLkA46bVW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import files\n",
        "import io\n",
        "\n",
        "negativas = files.upload()\n",
        "negativas\n",
        "neg_count = 0\n",
        "neg_correct = 0\n",
        "\n",
        "with open(\"negativas.txt\",\"r\") as f:\n",
        "  for line in f.read().split('\\n'):\n",
        "    analysis = TextBlob(line)\n",
        "    #print(line)\n",
        "    try:\n",
        "      eng=analysis.translate(to='en')\n",
        "      if eng.sentiment.polarity <= 0:\n",
        "        neg_correct += 1\n",
        "      neg_count +=1\n",
        "    except:\n",
        "      print('el elemento no esta presente')\n",
        "\n",
        "print(\"Precisión negativa = {}% via {} ejemplos\".format(neg_correct/neg_count*100.0, neg_count))\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "60jct21p43fg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tweepy import Stream\n",
        "from tweepy import OAuthHandler\n",
        "from tweepy.streaming import StreamListener\n",
        "\n",
        "#consumer key, consumer secret, access token, access secret.\n",
        "ckey=\"F2pIrutjymGr9vZuqTeViAymw\"\n",
        "csecret=\"QFLTXwFJZNuR6i00IswAZIgaKsKl5AtmPufoSaRnR57ER2yVxS\"\n",
        "atoken=\"879408865997201408-QJDeVNKYBsTdp97caK0qFV454YYwRLp\"\n",
        "asecret=\"gLNfeVRjT7LrDaWWSgCX0ZRu6TeTPBquTxlYKQ4hUzAka\"\n",
        "\n",
        "class listener(StreamListener):\n",
        "\n",
        "  def on_data(self, data):\n",
        "    print(data)\n",
        "    return(True)\n",
        "\n",
        "  def on_error(self, status):\n",
        "    print (status)\n",
        "\n",
        "auth = OAuthHandler(ckey, csecret)\n",
        "auth.set_access_token(atoken, asecret)\n",
        "\n",
        "twitterStream = Stream(auth, listener())\n",
        "twitterStream.filter(track=[\"python\"])\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y7Xjx5qRoTFd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tweepy import Stream\n",
        "from tweepy import OAuthHandler\n",
        "from tweepy.streaming import StreamListener\n",
        "import json\n",
        "import sqlite3\n",
        "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
        "from unidecode import unidecode\n",
        "import time\n",
        "\n",
        "analyzer = SentimentIntensityAnalyzer()\n",
        "\n",
        "#consumer key, consumer secret, access token, access secret.\n",
        "ckey=\"F2pIrutjymGr9vZuqTeViAymw\"\n",
        "csecret=\"QFLTXwFJZNuR6i00IswAZIgaKsKl5AtmPufoSaRnR57ER2yVxS\"\n",
        "atoken=\"879408865997201408-QJDeVNKYBsTdp97caK0qFV454YYwRLp\"\n",
        "asecret=\"gLNfeVRjT7LrDaWWSgCX0ZRu6TeTPBquTxlYKQ4hUzAka\"\n",
        "\n",
        "conn = sqlite3.connect('twitterF1.db')\n",
        "c = conn.cursor()\n",
        "\n",
        "def create_table():\n",
        "  try:\n",
        "    c.execute(\"CREATE TABLE IF NOT EXISTS sentimiento(unix REAL, tweet TEXT, sentimiento REAL)\")\n",
        "    c.execute(\"CREATE INDEX fast_unix ON sentimiento(unix)\")\n",
        "    c.execute(\"CREATE INDEX fast_tweet ON sentimiento(tweet)\")\n",
        "    c.execute(\"CREATE INDEX fast_sentimiento ON sentimiento(sentimiento)\")\n",
        "    conn.commit()\n",
        "  except Exception as e:\n",
        "    print(str(e))\n",
        "create_table()\n",
        "\n",
        "class listener(StreamListener):\n",
        "\n",
        "  def on_data(self, data):\n",
        "    try:\n",
        "      data = json.loads(data)\n",
        "      tweet = unidecode(data['text'])\n",
        "      time_ms = data['timestamp_ms']\n",
        "      vs = analyzer.polarity_scores(tweet)\n",
        "      sentimiento = vs['compound']\n",
        "      print(time_ms, tweet, sentimiento)\n",
        "      c.execute(\"INSERT INTO sentimiento (unix, tweet, sentimiento) VALUES (?, ?, ?)\",\n",
        "            (time_ms, tweet, sentimiento))\n",
        "      conn.commit()\n",
        "\n",
        "    except KeyError as e:\n",
        "      print(str(e))\n",
        "    return(True)\n",
        "\n",
        "  def on_error(self, status):\n",
        "    print(status)\n",
        "\n",
        "while True:\n",
        "\n",
        "  try:\n",
        "    auth = OAuthHandler(ckey, csecret)\n",
        "    auth.set_access_token(atoken, asecret)\n",
        "    twitterStream = Stream(auth, listener())\n",
        "    twitterStream.filter(track=[\"a\",\"e\",\"i\",\"o\",\"u\"])\n",
        "  except Exception as e:\n",
        "    print(str(e))\n",
        "    time.sleep(5)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PDHzMZIPuuJb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import sqlite3\n",
        "import pandas as pd\n",
        "\n",
        "conn = sqlite3.connect('twitterF1.db')\n",
        "c = conn.cursor()\n",
        "\n",
        "df = pd.read_sql(\"SELECT * FROM sentimiento WHERE tweet LIKE '%py%' ORDER BY unix DESC LIMIT 1000\", conn)\n",
        "df.sort_values('unix', inplace=True)\n",
        "\n",
        "df['sentimiento_smoothed'] = df['sentimiento'].rolling(int(len(df)/5)).mean()\n",
        "df.dropna(inplace=True)\n",
        "print(df.tail())\n"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}